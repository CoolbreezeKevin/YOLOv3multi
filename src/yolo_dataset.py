# Copyright 2020-2022 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""YOLOV3 dataset."""
import os

import multiprocessing
import cv2
from PIL import Image
import numpy as np
from pycocotools.coco import COCO
import mindspore.dataset as ds

from src.distributed_sampler import DistributedSampler
from src.transforms import reshape_fn, MultiScaleTrans
from mindspore.dataset import CocoDataset

min_keypoints_per_image = 10


def _has_only_empty_bbox(anno):
    return all(any(o <= 1 for o in obj["bbox"][2:]) for obj in anno)


def _count_visible_keypoints(anno):
    return sum(sum(1 for v in ann["keypoints"][2::3] if v > 0) for ann in anno)


def has_valid_annotation(anno):
    """Check annotation file."""
    # if it's empty, there is no annotation
    if not anno:
        return False
    # if all boxes have close to zero area, there is no annotation
    if _has_only_empty_bbox(anno):
        return False
    # keypoints task have a slight different criteria for considering
    # if an annotation is valid
    if "keypoints" not in anno[0]:
        return True
    # for keypoint detection tasks, only consider valid images those
    # containing at least min_keypoints_per_image
    if _count_visible_keypoints(anno) >= min_keypoints_per_image:
        return True
    return False


class COCOYoloDataset:
    """YOLOV3 Dataset for COCO."""
    def __init__(self, root, ann_file, cfg=None, remove_images_without_annotations=False,
                 filter_crowd_anno=True, is_training=True):
        self.coco = COCO(ann_file)
        self.root = root
        self.img_ids = list(sorted(self.coco.imgs.keys()))
        self.filter_crowd_anno = filter_crowd_anno
        self.is_training = is_training
        self.cfg =cfg
        # filter images without any annotations
        if remove_images_without_annotations:
            img_ids = []
            for img_id in self.img_ids:
                ann_ids = self.coco.getAnnIds(imgIds=img_id, iscrowd=None)
                anno = self.coco.loadAnns(ann_ids)
                if has_valid_annotation(anno):
                    img_ids.append(img_id)
            self.img_ids = img_ids

        self.categories = {cat["id"]: cat["name"] for cat in self.coco.cats.values()}

        self.cat_ids_to_continuous_ids = {
            v: i for i, v in enumerate(self.coco.getCatIds())
        }
        self.continuous_ids_cat_ids = {
            v: k for k, v in self.cat_ids_to_continuous_ids.items()
        }

    def __getitem__(self, index):
        """
        Args:
            index (int): Index

        Returns:
            (img, target) (tuple): target is a dictionary contains "bbox", "segmentation" or "keypoints",
                generated by the image's annotation. img is a PIL image.
        """
        coco = self.coco
        img_id = self.img_ids[index]
        img_path = coco.loadImgs(img_id)[0]["file_name"]
        if not self.is_training:
            img = Image.open(os.path.join(self.root, img_path)).convert("RGB")
            return img, img_id
        img = np.fromfile(os.path.join(self.root, img_path), dtype="int8")

        ann_ids = coco.getAnnIds(imgIds=img_id)
        target = coco.loadAnns(ann_ids)
        # filter crowd annotations
        if self.filter_crowd_anno:
            annos = [anno for anno in target if anno["iscrowd"] == 0]
        else:
            annos = [anno for anno in target]

        target = {}
        boxes = [anno["bbox"] for anno in annos]
        target["bboxes"] = boxes

        classes = [anno["category_id"] for anno in annos]
        classes = [self.cat_ids_to_continuous_ids[cl] for cl in classes]
        target["labels"] = classes

        bboxes = target['bboxes']
        labels = target['labels']
        out_target = []
        for bbox, label in zip(bboxes, labels):
            tmp = []
            # convert to [x_min y_min x_max y_max]
            bbox = self._convetTopDown(bbox)
            tmp.extend(bbox)
            tmp.append(int(label))
            # tmp [x_min y_min x_max y_max, label]
            out_target.append(tmp)
        return img, out_target, [], [], [], [], [], []

    def __len__(self):
        return len(self.img_ids)

    def _convetTopDown(self, bbox):
        x_min = bbox[0]
        y_min = bbox[1]
        w = bbox[2]
        h = bbox[3]
        return [x_min, y_min, x_min+w, y_min+h]

class COCOYoloDatasetWithSeg(COCOYoloDataset):
    def __init__(self, *arg, **kwargs):
        super().__init__(*arg, **kwargs)
        self.seg_path= self.cfg.seg_path
    def __getitem__(self, index):
        """
        Args:
            index (int): Index

        Returns:
            (img, target) (tuple): target is a dictionary contains "bbox", "segmentation" or "keypoints",
                generated by the image's annotation. img is a PIL image.
        """
        coco = self.coco
        img_id = self.img_ids[index]
        img_path = coco.loadImgs(img_id)[0]["file_name"]

        img = np.fromfile(os.path.join(self.root, img_path), dtype="int8")
        mask_path = os.path.join(self.seg_path, img_path.replace(".tif",".png"))        
        mask = cv2.imread(mask_path, -1)
        # mask = np.expand_dims(mask,2)
        # print()
        # img=np.concatenate((img,mask),2)
        if not self.is_training:
            img = Image.open(os.path.join(self.root, img_path)).convert("RGB")
            return img, img_id, [mask]
        
        ann_ids = coco.getAnnIds(imgIds=img_id)
        target = coco.loadAnns(ann_ids)
        # filter crowd annotations
        if self.filter_crowd_anno:
            annos = [anno for anno in target if anno["iscrowd"] == 0]
        else:
            annos = [anno for anno in target]

        target = {}
        boxes = [anno["bbox"] for anno in annos]
        target["bboxes"] = boxes

        classes = [anno["category_id"] for anno in annos]
        classes = [self.cat_ids_to_continuous_ids[cl] for cl in classes]
        target["labels"] = classes

        bboxes = target['bboxes']
        labels = target['labels']
        out_target = []
        for bbox, label in zip(bboxes, labels):
            tmp = []
            # convert to [x_min y_min x_max y_max]
            bbox = self._convetTopDown(bbox)
            tmp.extend(bbox)
            tmp.append(int(label))
            # tmp [x_min y_min x_max y_max, label]
            out_target.append(tmp)
        # out_target.append()
        return img, out_target,[mask], [], [], [], [], [], []

def generate_csv(js_path, csv_path ="./folds.csv", fold_num = 5):
    coco2 = json.load(open(js_path,'r'))
    
    lnth = len(coco2["images"])
    fold_len = lnth//fold_num

    names = []
    fold = []
    ids = []
    for i, im in enumerate(coco2["images"]):
        f = i // fold_len
        names.append(im["file_name"])
        fold.append(f)
        ids.append(im['id'])

    with open(csv_path, 'w') as f:
        f.write('id,name,fold\n')
        for id, name, fold in zip(ids, names, fold):
            f.write(str(id)+','+name+','+str(fold)+'\n')

class FoldTrainWithSeg(COCOYoloDatasetWithSeg):
    def __init__(self, *arg, folds=[0,1,2,3], **kwargs):
        super().__init__(*arg, **kwargs)
        self.folds = folds if not self.cfg.folds else self.cfg.folds
        self.folds = set(self.folds)
        
        if not self.cfg.fold_csv:
            generate_csv(self.cfg.ann_file)
            self.csvpth = './folds.csv' 
        else:
            self.csvpth = self.cfg.fold_csv
        df = pd.read_csv(self.csvpth)
        img_ids = []
        
        for img_id in self.img_ids:
            f = df[df['id']==img_id]['fold'].tolist()[0]
            if f in folds:
                img_ids.append(img_id)
        self.img_ids = img_ids

        self.categories = {cat["id"]: cat["name"] for cat in self.coco.cats.values()}

        self.cat_ids_to_continuous_ids = {
            v: i for i, v in enumerate(self.coco.getCatIds())
        }
        self.continuous_ids_cat_ids = {
            v: k for k, v in self.cat_ids_to_continuous_ids.items()
        }

    
dataset_dict = {
    "COCOYoloDatasetWithSeg":COCOYoloDatasetWithSeg,
    "COCOYoloDataset":COCOYoloDatasetï¼Œ
    "FoldTrainWithSeg":FoldTrainWithSeg
}

def create_yolo_dataset(image_dir, anno_path, batch_size, device_num, rank,
                        config=None, is_training=True, shuffle=True):
    """Create dataset for YOLOV3."""
    cv2.setNumThreads(0)

    if is_training:
        filter_crowd = True
        remove_empty_anno = True
    else:
        filter_crowd = False
        remove_empty_anno = False
    DataSet = dataset_dict[config.datasettype]

    yolo_dataset = DataSet(root=image_dir, ann_file=anno_path, cfg=config, filter_crowd_anno=filter_crowd,
                                   remove_images_without_annotations=remove_empty_anno, is_training=is_training)
    hwc_to_chw = ds.vision.HWC2CHW()

    config.dataset_size = len(yolo_dataset)
    cores = multiprocessing.cpu_count()
    num_parallel_workers = int(cores / device_num)
    distributed_sampler = DistributedSampler(len(yolo_dataset), device_num, rank, shuffle=shuffle)
    if is_training:
        multi_scale_trans = MultiScaleTrans(config, device_num)
        dataset_column_names = ["image", "annotation","seg", "bbox1", "bbox2", "bbox3",
                                "gt_box1", "gt_box2", "gt_box3"]
        # dataset_column_names_out = ["image", "annotation", "bbox1", "bbox2", "bbox3",
        #                         "gt_box1", "gt_box2", "gt_box3","seg"]
        # dataset_column_names = None         
        if device_num != 8:
            dataset = ds.GeneratorDataset(yolo_dataset, column_names=dataset_column_names, sampler=distributed_sampler)
            dataset = dataset.map(operations=ds.vision.Decode(), input_columns=["image"])
            dataset = dataset.batch(batch_size, per_batch_map=multi_scale_trans, input_columns=dataset_column_names,
                                    num_parallel_workers=min(32, num_parallel_workers), drop_remainder=True)
        else:
            dataset = ds.GeneratorDataset(yolo_dataset, column_names=dataset_column_names, sampler=distributed_sampler)
            dataset = dataset.map(operations=ds.vision.Decode(), input_columns=["image"])
            dataset = dataset.batch(batch_size, per_batch_map=multi_scale_trans, input_columns=dataset_column_names,
                                    num_parallel_workers=min(8, num_parallel_workers), drop_remainder=True)
    else:
        valid_dataset = FoldTrainWithSeg(root=image_dir, ann_file=anno_path, cfg=config, filter_crowd_anno=filter_crowd,
                                   remove_images_without_annotations=remove_empty_anno, is_training=is_training, folds=config.valid_folds)
        hwc_to_chw = ds.vision.HWC2CHW()
        # dataset_column_names
        config.dataset_size = len(yolo_dataset)
        config.valid_dataset_size = len(valid_dataset)
        cores = multiprocessing.cpu_count()
        num_parallel_workers = int(cores / device_num)
        distributed_sampler = DistributedSampler(len(yolo_dataset), device_num, rank, shuffle=shuffle)

        # distributed_sampler = DistributedSampler(len(valid_dataset), device_num, rank, shuffle=shuffle)
        dataset = ds.GeneratorDataset(yolo_dataset, column_names=["image", "img_id", "masks"], sampler=distributed_sampler)

        compose_map_func = (lambda image, img_id, masks: reshape_fn(image, img_id, masks, config))
        dataset = dataset.map(operations=compose_map_func, input_columns=["image", "img_id","masks"],
                              output_columns=["image", "image_shape", "img_id", "masks"],
                              column_order=["image", "image_shape", "img_id", "masks"],
                              num_parallel_workers=8)
        dataset = dataset.map(operations=hwc_to_chw, input_columns=["image"], num_parallel_workers=8)
        # dataset = dataset.map(operations=hwc_to_chw, input_columns=["masks"], num_parallel_workers=8)
        dataset = dataset.batch(batch_size, drop_remainder=True)

    return dataset

def checkvalid(img):
    unq = np.unique(img)
    if unq.shape[0]<=2:
        return False
    return True

def statistic_normalize_img(img, idx):

    flag = checkvalid(img)

    """Statistic normalize images."""
    # img: RGB
    if isinstance(img, Image.Image):
        img = np.array(img)
    img = img/255.
    # Computed from random subset of ImageNet training images
    mean = np.array([0.485, 0.456, 0.406])
    std = np.array([0.229, 0.224, 0.225])
    img = (img - mean) / std
    return img.astype('float32'), idx, flag

class TestTimeDataset:
    def __init__(self, img_dir, cocofile) -> None:
        self.coco = COCO(cocofile)
        self.img_dir = img_dir
        self.categories = {cat["id"]: cat["name"] for cat in self.coco.cats.values()}
        self.img_ids = list(sorted(self.coco.imgs.keys()))
    def __getitem__(self, index):
        coco = self.coco
        img_id = self.img_ids[index]
        img_path = coco.loadImgs(img_id)[0]["file_name"]
        img = np.fromfile(os.path.join(self.img_dir, img_path), dtype="uint8")
        img = Image.open(os.path.join(self.img_dir, img_path)).convert("RGB")
        return img, img_id
    def __len__(self):
        return len(self.img_ids)


def create_test_dataset(img_dir, cocofile, rank=0, group_size=1):
    hwc_to_chw = ds.vision.HWC2CHW()
    dataset = TestTimeDataset(img_dir, cocofile)
    distributed_sampler = DistributedSampler(len(dataset), group_size, rank, shuffle=False)
    compose_map_func = (lambda image, img_id: statistic_normalize_img(image, img_id))
    dataset = ds.GeneratorDataset(dataset, column_names=["image", "img_id"], sampler=distributed_sampler)
    # dataset = dataset.map(operations=hwc_to_chw, input_columns=["image"], num_parallel_workers=8)
    
    dataset = dataset.map(operations=compose_map_func, input_columns=["image", "img_id"],
                            output_columns=["image", "img_id", 'flag'], column_order=["image", "img_id", 'flag'],
                            num_parallel_workers=2)
    dataset = dataset.map(operations=hwc_to_chw, input_columns=["image"],
                          num_parallel_workers=2)
    dataset = dataset.batch(1, drop_remainder=False)         
    return dataset
